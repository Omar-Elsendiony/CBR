{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "3UrPezIyhz_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq torch"
      ],
      "metadata": {
        "id": "0dBkqvdb7Fj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbykauYOwY7Z",
        "outputId": "01ea1cdf-3b31-4a45-be0f-337033cc3fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/sun-attribute\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"jiangfeilong/sun-attribute\")\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "try:\n",
        "    subprocess.run([\"cp\", \"-r\", \"/root/.cache/kagglehub/datasets/jiangfeilong/sun-attribute/versions/1\", \".\"], check=True)\n",
        "except Exception as e:\n",
        "    subprocess.run([\"cp\", \"-r\", \"/kaggle/input/sun-attribute\", \".\"], check=True)"
      ],
      "metadata": {
        "id": "kAQlRUa1xZJV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "try:\n",
        "    os.chdir(\"1/SUN/SUNAttributeDB/\")\n",
        "except FileNotFoundError:\n",
        "    os.chdir(\"sun-attribute/SUN/SUNAttributeDB/\")"
      ],
      "metadata": {
        "id": "QEf-4_7JiyUa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Extract attribute labels for images\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "\n",
        "# Load the .mat file\n",
        "images_paths = scipy.io.loadmat('images.mat')\n",
        "images_paths = images_paths['images']\n",
        "\n",
        "labels_attr = scipy.io.loadmat('attributeLabels_continuous.mat')\n",
        "labels_attr = labels_attr['labels_cv']\n",
        "print(len(labels_attr))  # If it's an array\n",
        "\n",
        "attribute_names = scipy.io.loadmat('attributes.mat')\n",
        "attribute_names = attribute_names['attributes']\n",
        "\n",
        "print(len(attribute_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKePNHs7Nwm8",
        "outputId": "00432998-b945-4af5-d40c-e22750c49157",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14340\n",
            "102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create Image Metadata\n",
        "\n",
        "def create_list_of_dictionary_dataset(images_data, labels_attr):\n",
        "  dataset_list = []\n",
        "  for i in range(len(images_data)):\n",
        "    data_dict = dict()\n",
        "    path = str(images_data[i][0][0])\n",
        "\n",
        "    data_dict['path'] = path\n",
        "    splitted_path = path.split('/')\n",
        "    if len(splitted_path) == 3:\n",
        "      category = splitted_path[-2]\n",
        "    elif len(splitted_path) == 4:\n",
        "      category = '_'.join(splitted_path[-3:-1])\n",
        "      # print(category)\n",
        "    else:\n",
        "      print(\"error\")\n",
        "\n",
        "    data_dict['label'] = category\n",
        "    data_dict['labels_attr'] = labels_attr[i]\n",
        "\n",
        "    dataset_list.append(data_dict)\n",
        "  return dataset_list\n",
        "\n",
        "\n",
        "images_metadata = create_list_of_dictionary_dataset(images_paths, labels_attr)"
      ],
      "metadata": {
        "id": "9_EhcKueu2QZ",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmyixUIfCwkG",
        "outputId": "85ebf7b5-1806-4619-dfd0-7557683fb6c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/sun-attribute/SUN/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create Test Set\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "moved_files_set = set()\n",
        "\n",
        "test_images_dir = \"../test_images\"\n",
        "# Create the test_images directory if it doesn't exist\n",
        "if not os.path.exists(test_images_dir):\n",
        "    os.mkdir(test_images_dir)\n",
        "\n",
        "def extract_test_set(root_path):\n",
        "    \"\"\"\n",
        "    Extracts a percentage of files from a nested directory structure\n",
        "    into a new 'test_images' directory, preserving category names.\n",
        "\n",
        "    It moves files from third-level directories if they exist.\n",
        "    If no third-level directories exist within a second-level directory,\n",
        "    it moves files directly from the second-level directory.\n",
        "\n",
        "    Args:\n",
        "        root_path (str): The root directory to start scanning from.\n",
        "\n",
        "    Returns:\n",
        "        int: The total number of files moved.\n",
        "    \"\"\"\n",
        "    number_files_moved = 0\n",
        "\n",
        "    # Iterate through the first level directories\n",
        "    for first_level in os.listdir(root_path):\n",
        "        first_level_path = os.path.join(root_path, first_level)\n",
        "\n",
        "        # Ensure it's a directory before proceeding\n",
        "        if os.path.isdir(first_level_path):\n",
        "            if (first_level == \"misc\" or first_level == \"outliers\"): continue;\n",
        "            # Iterate through the second level directories within the first level\n",
        "            for second_level in os.listdir(first_level_path):\n",
        "                second_level_path = os.path.join(first_level_path, second_level)\n",
        "                second_level_category = second_level # Use second level name for base category\n",
        "\n",
        "                # Ensure it's a directory before proceeding\n",
        "                if os.path.isdir(second_level_path):\n",
        "\n",
        "                    # Check if there are any third-level directories within the second level\n",
        "                    third_level_dirs = [\n",
        "                        d for d in os.listdir(second_level_path)\n",
        "                        if os.path.isdir(os.path.join(second_level_path, d))\n",
        "                    ]\n",
        "\n",
        "                    # --- Logic to decide whether to process second or third level ---\n",
        "\n",
        "                    if third_level_dirs:\n",
        "                        # Process files in the third level directories if they exist\n",
        "                        for third_level in third_level_dirs:\n",
        "                            third_level_path = os.path.join(second_level_path, third_level)\n",
        "                            # Create category name combining second and third levels\n",
        "                            third_level_category_name = f\"{second_level_category}_{third_level}\"\n",
        "\n",
        "                            # Get list of files in the third level directory\n",
        "                            third_level_files = [\n",
        "                                f for f in os.listdir(third_level_path)\n",
        "                                if os.path.isfile(os.path.join(third_level_path, f))\n",
        "                            ]\n",
        "\n",
        "                            if third_level_files:\n",
        "                                # Create the destination category directory\n",
        "                                category_dir = os.path.join(test_images_dir, third_level_category_name)\n",
        "                                if not os.path.exists(category_dir):\n",
        "                                    os.mkdir(category_dir)\n",
        "\n",
        "                                # Determine number of files to move (at least 1, or 10%)\n",
        "                                number_files_to_move = max(int(len(third_level_files) * 0.1), 1)\n",
        "\n",
        "                                # Move the selected files\n",
        "                                for file in third_level_files[:number_files_to_move]:\n",
        "                                    file_to_move = os.path.join(third_level_path, file)\n",
        "                                    destination_path = os.path.join(category_dir, file) # Define full destination path\n",
        "                                    try:\n",
        "                                        shutil.move(file_to_move, destination_path)\n",
        "                                        moved_file = file_to_move[2:]\n",
        "                                        moved_files_set.add(moved_file) # Add destination path to set\n",
        "                                        number_files_moved += 1 # Increment count for each file moved\n",
        "                                        # print(f\"Moved: {file_to_move} -> {destination_path}\")\n",
        "                                    except Exception as e:\n",
        "                                        print(f\"Error moving file {file_to_move}: {e}\")\n",
        "\n",
        "                    else:\n",
        "                        # Process files directly in the second level if NO third-level directories exist\n",
        "                        second_level_files = [\n",
        "                            f for f in os.listdir(second_level_path)\n",
        "                            if os.path.isfile(os.path.join(second_level_path, f))\n",
        "                        ]\n",
        "\n",
        "                        if second_level_files:\n",
        "                            # Create the destination category directory using the second level name\n",
        "                            category_dir = os.path.join(test_images_dir, second_level_category)\n",
        "                            if not os.path.exists(category_dir):\n",
        "                                os.mkdir(category_dir)\n",
        "\n",
        "                            # Determine number of files to move (at least 1, or 10%)\n",
        "                            number_files_to_move = max(int(len(second_level_files) * 0.1), 1)\n",
        "\n",
        "                            # Move the selected files\n",
        "                            for file in second_level_files[:number_files_to_move]:\n",
        "                                file_to_move = os.path.join(second_level_path, file)\n",
        "                                destination_path = os.path.join(category_dir, file) # Define full destination path\n",
        "                                try:\n",
        "                                    shutil.move(file_to_move, destination_path)\n",
        "\n",
        "                                    moved_file = file_to_move[2:]\n",
        "                                    moved_files_set.add(moved_file)\n",
        "                                    number_files_moved += 1 # Increment count for each file moved\n",
        "                                    # print(f\"Moved: {file_to_move} -> {destination_path}\")\n",
        "                                except Exception as e:\n",
        "                                    print(f\"Error moving file {file_to_move}: {e}\")\n",
        "\n",
        "    return number_files_moved\n",
        "\n",
        "\n",
        "number_files_moved = extract_test_set(\".\")\n",
        "\n",
        "print(f\"\\nTotal files moved: {number_files_moved}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lDTAbFe4eX9",
        "outputId": "0163556a-e71c-43ea-e492-13ae5d7a5abc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total files moved: 1440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../test_images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGCg_50-4mf2",
        "outputId": "dc1625ef-fc93-48f3-9293-a7b52b6c84ce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/sun-attribute/SUN/test_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Validating testset creation and removing directories of one element\n",
        "# images_paths = scipy.io.loadmat('att_splits.mat')\n",
        "\n",
        "# allclasses_ds = set()\n",
        "# for allclasses in images_paths['allclasses_names']:\n",
        "#   # print(allclasses[0][0])\n",
        "#   allclasses_ds.add(allclasses[0][0])\n",
        "\n",
        "# # images_paths.__getitem__('train_loc').shape\n",
        "\n",
        "import os\n",
        "\n",
        "def count_directories(directory_path=\".\"):\n",
        "  \"\"\"\n",
        "  Counts the number of subdirectories within a given directory.\n",
        "\n",
        "  Args:\n",
        "    directory_path (str): The path to the directory to examine.\n",
        "                          Defaults to the current directory (.).\n",
        "\n",
        "  Returns:\n",
        "    int: The number of directories found, or -1 if the directory doesn't exist\n",
        "         or is not accessible.\n",
        "  \"\"\"\n",
        "  empty_dir = {\"roller_skating_rink_indoor\", \"police_station\", \"volleyball_court_indoor\", \"distillery\", \"barbershop\", \"ice_cream_parlor\"}\n",
        "  count = 0\n",
        "  try:\n",
        "    # Use os.scandir for better performance and getting file type info directly\n",
        "    with os.scandir(directory_path) as entries:\n",
        "      for entry in entries:\n",
        "\n",
        "        theEntry = entry.name\n",
        "        if (theEntry in empty_dir): shutil.rmtree(theEntry); continue\n",
        "        # Check if the entry is a directory (and not a symbolic link to a directory)\n",
        "        if entry.is_dir() and not entry.is_symlink():\n",
        "          count += 1\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: Directory not found at '{directory_path}'\")\n",
        "    return -1\n",
        "  except PermissionError:\n",
        "    print(f\"Error: Permission denied to access '{directory_path}'\")\n",
        "    return -1\n",
        "  except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "    return -1\n",
        "\n",
        "  return count\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "# Count directories in the current directory\n",
        "current_dir_count = count_directories(\".\")\n",
        "if current_dir_count != -1:\n",
        "  print(f\"Number of directories in the current directory: {current_dir_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVVHkt_W5ZV-",
        "outputId": "f814f518-5dda-47c1-85f7-2ee95a94fda5",
        "cellView": "form"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of directories in the current directory: 717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Images labels for Dataloader\n",
        "images_labels = {example['path']:example['labels_attr'].tolist() for example in images_metadata if (example['path']) not in moved_files_set}"
      ],
      "metadata": {
        "id": "fjEmXzTU5m-L"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(images_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLA0qUwaweIE",
        "outputId": "d8ba00d6-0029-4be3-ca41-d374e9833b80"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12940\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "rw3iQOuKhw9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# loc_dict = dict()\n",
        "# def traverse_directory(root_dir):\n",
        "#     # print(\"hello\")\n",
        "#     for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "#         # print(filenames)\n",
        "#         # print(dirpath)\n",
        "#         # Skip the root directory itself\n",
        "#         for d in dirnames:\n",
        "#           # print(d)\n",
        "#           for dirpath, dirnames, filenames in os.walk(os.path.join(root_dir,d)):\n",
        "#             if dirpath == root_dir:\n",
        "#                 continue\n",
        "\n",
        "#             # Print the current directory path\n",
        "#             # print(f\"\\n{dirpath}:\")\n",
        "\n",
        "#             # Print all files in this directory\n",
        "#             for filename in sorted(filenames):\n",
        "#                 # print(f\"  {filename}\")\n",
        "#                 loc_dict[filename] = dirpath.split('/')[-1]\n",
        "\n",
        "# # Example usage:\n",
        "# if __name__ == \"__main__\":\n",
        "#     directory_to_scan = \"./1/SUN/images\"  # Change this to your directory\n",
        "#     traverse_directory(directory_to_scan)\n",
        "\n",
        "# print((loc_dict))"
      ],
      "metadata": {
        "id": "V4lYM5liAgfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Dense Features of an image"
      ],
      "metadata": {
        "id": "wlDEvTlaIN9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Extract Dense Features P1\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Initialize model\n",
        "alexnet = models.alexnet(weights='AlexNet_Weights.DEFAULT')\n",
        "feature_extractor = nn.Sequential(*list(alexnet.classifier.children())[:-2])\n",
        "\n",
        "class CNNFeatureExtractor(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.features = model.features\n",
        "        self.avgpool = model.avgpool\n",
        "        self.classifier = feature_extractor\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x  # Shape: [batch_size, 4096]\n",
        "\n",
        "# Define transforms\n",
        "alexnet_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Initialize feature extractor\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNNFeatureExtractor(alexnet).to(device)\n",
        "model.eval()\n",
        "\n",
        "def process_image(image_path):\n",
        "    \"\"\"Process a single image and return features with label\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    img_tensor = alexnet_transforms(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model(img_tensor)  # shape: [1, 4096]\n",
        "        return features.cpu().numpy().flatten()"
      ],
      "metadata": {
        "id": "6-2_QhP4ISAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "222bcd42-382f-4fb1-a15d-fcfe1ead24f0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:01<00:00, 163MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIr8qRbtvedz",
        "outputId": "1ef12a14-12d2-4935-bf78-a3ae0d3fccda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/1/SUN/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Extract Dense Features P2\n",
        "\n",
        "images_deep_features = list()\n",
        "\n",
        "# Process a single image\n",
        "# image_path = \"./1/SUN/images/a/abbey/sun_aacphuqehdodwawg.jpg\"\n",
        "# label = \"abbey\"\n",
        "# label, features = process_image(image_path, label)\n",
        "# features_dict[label] = features\n",
        "\n",
        "i = 0\n",
        "for d in images_metadata:\n",
        "  label = d['label']\n",
        "  image_path = os.path.join('.', d['path'])\n",
        "  try:\n",
        "    features = process_image(image_path)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  features_dict = dict()\n",
        "  # features_dict[image_path] =\n",
        "  images_deep_features.append({\n",
        "    'path': image_path,\n",
        "    'label': label,\n",
        "    'features_vector': features\n",
        "  })\n",
        "\n",
        "  i += 1\n",
        "  print(\"Reached: \", i)\n",
        "  if (i == 500):\n",
        "    break"
      ],
      "metadata": {
        "id": "iPA_Bkw1Hco4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New Approach"
      ],
      "metadata": {
        "id": "f0z6kXjLDnkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class AttributePredictor(nn.Module):\n",
        "    def __init__(self, num_attributes=102):\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet152(pretrained=True)\n",
        "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_attributes)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sigmoid(self.backbone(x))\n",
        "\n",
        "# Initialize model, optimizer, and loss function\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_res = AttributePredictor(num_attributes=102).to(device)\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for multi-label\n",
        "optimizer = torch.optim.Adam(model_res.parameters(), lr=0.001)\n",
        "\n",
        "# Data preparation (example)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Training function\n",
        "def train(model, dataloader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).int()\n",
        "            total += labels.size(0) * labels.size(1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        epoch_acc = correct / total\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).int()\n",
        "            total += labels.size(0) * labels.size(1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_loss = running_loss / len(dataloader)\n",
        "    test_acc = correct / total\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "    return test_loss, test_acc\n",
        "\n",
        "# Inference for a single image\n",
        "def predict_attributes(model, image_path, transform):\n",
        "    from PIL import Image\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)\n",
        "        predicted_attributes = (output > 0.5).int()  # Binarize to 0/1\n",
        "\n",
        "    return predicted_attributes\n",
        "\n",
        "# Example usage:\n",
        "# train(model, train_dataloader, criterion, optimizer, epochs=10)\n",
        "# test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
        "# predicted_attrs = predict_attributes(model, \"path/to/image.jpg\", transform)"
      ],
      "metadata": {
        "id": "Tdg68DPiJ2hz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6445e373-18ee-4738-d004-859d7e0d06ee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
            "100%|██████████| 230M/230M [00:01<00:00, 172MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training and Evaluation\n",
        "train(model, train_dataloader, criterion, optimizer, epochs=40)\n",
        "evaluate(model, val_dataloader, criterion)"
      ],
      "metadata": {
        "id": "-L8S_70m7-PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hms1-a3u8Cf5",
        "outputId": "a58eadc8-0090-425c-a696-e0228818912b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/1/SUN/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_vector_example = process_image('../test_images/abbey/sun_ajblooupyqwdvzgx.jpg')"
      ],
      "metadata": {
        "id": "MRErq-Cw7i4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = process_image('../test_images/airport_entrance/sun_asimhuxhlemcrozs.jpg')\n",
        "zero_percentage = (features == 0).sum() / features.size * 100\n",
        "print(f\"Percentage of zeros: {zero_percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYrffHlk9GI9",
        "outputId": "6dda93d3-e2bf-4585-9596-b04351ce99c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of zeros: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(features[1990])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSWFjwil9Yjm",
        "outputId": "edb03136-b2c2-4705-a2b3-95397939e809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-14.041405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create DataLoaders and Datasets"
      ],
      "metadata": {
        "id": "kpWuQvp0jM0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "\n",
        "class AttributeDataset(Dataset):\n",
        "    def __init__(self, image_dir, label_dict, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dict = label_dict\n",
        "        self.image_files = list(label_dict.keys())\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = torch.tensor(self.label_dict[img_name], dtype=torch.float32)\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "X4O0-RYN2IWj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeSir-w-XfDZ",
        "outputId": "ce6c0e46-c953-4891-9617-67b71826f15d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/sun-attribute/SUN/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(images_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzWMPWsN4Mpx",
        "outputId": "96bcced6-c6a8-4481-e44a-b84486f26084"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12940\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set this to your actual folder\n",
        "image_dir = \".\"\n",
        "\n",
        "# Use your transform (same as the one you already defined)\n",
        "dataset = AttributeDataset(image_dir=image_dir, label_dict=images_labels, transform=transform)\n",
        "\n",
        "# Create train/test splits if needed\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_size = int(0.99 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "FRGwj4ur12R-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming train_dataset is the Subset object created by random_split\n",
        "num_train_samples = len(train_dataset)\n",
        "\n",
        "print(f\"The number of elements in the training dataset is: {num_train_samples}\")\n",
        "\n",
        "\n",
        "num_train_samples = len(val_dataset)\n",
        "\n",
        "print(f\"The number of elements in the training dataset is: {num_train_samples}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyGzTBB2H7HL",
        "outputId": "1b94674d-b3d8-4f97-d3d6-34f3c2830c35"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of elements in the training dataset is: 10352\n",
            "The number of elements in the training dataset is: 2588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pJyaCcK3-rQ",
        "outputId": "bf9e996f-6913-4363-d95e-3cda0ed8e010"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a  c  e  g  i  k  m\tn  outliers  q\ts  u  w  z\n",
            "b  d  f  h  j  l  misc\to  p\t     r\tt  v  y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Content of one batch from train_dataloader ---\")\n",
        "if len(train_dataloader) > 0:\n",
        "    # Get one batch of data\n",
        "    batch_images, batch_labels = next(iter(train_dataloader)) # Assuming dataloader yields (images, labels)\n",
        "\n",
        "    print(f\"Batch Image Tensor Shape: {batch_images.shape}\") # e.g., [32, C, H, W]\n",
        "    print(f\"Batch Labels Shape: {batch_labels.shape}\")     # e.g., [32] or [32, num_attributes]\n",
        "    print(f\"Sample Labels from Batch: {batch_labels[:1]}...\") # Show first 5 labels in the batch\n",
        "else:\n",
        "    print(\"Train dataloader is empty or could not iterate.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9xBE4EGJJqm",
        "outputId": "218f209c-17e2-4673-dae0-3c29c686f6fb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Content of one batch from train_dataloader ---\n",
            "Batch Image Tensor Shape: torch.Size([32, 3, 224, 224])\n",
            "Batch Labels Shape: torch.Size([32, 102])\n",
            "Sample Labels from Batch: tensor([[0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.6667, 0.0000, 0.6667, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6667, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.3333, 0.0000, 0.3333]])...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title deprecated\n",
        "# import os\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from PIL import Image\n",
        "# from torch.utils.data import Dataset, DataLoader, random_split\n",
        "# from torchvision import transforms\n",
        "# import glob\n",
        "\n",
        "# class SUNDataset(Dataset):\n",
        "#     def __init__(self, root_dir, transform=None):\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             root_dir (string): Root directory with images organized in subdirectories by class\n",
        "#             transform (callable, optional): Optional transform to be applied on images\n",
        "#         \"\"\"\n",
        "#         self.root_dir = root_dir\n",
        "#         self.transform = transform\n",
        "#         self.classes = []\n",
        "#         self.class_to_idx = {}\n",
        "#         self.images = []\n",
        "#         self.labels = []\n",
        "\n",
        "#         # Find all class directories (scene categories)\n",
        "#         for path, dirs, files in os.walk(root_dir):\n",
        "#             # Get only the innermost directories that contain images\n",
        "#             if files and any(f.lower().endswith(('.jpg', '.jpeg', '.png')) for f in files):\n",
        "#                 # Extract class name from path\n",
        "#                 class_name = os.path.basename(path)\n",
        "\n",
        "#                 # Add to class list if not already there\n",
        "#                 if class_name not in self.class_to_idx:\n",
        "#                     self.class_to_idx[class_name] = len(self.classes)\n",
        "#                     self.classes.append(class_name)\n",
        "\n",
        "#                 # Add all images from this class\n",
        "#                 for img_file in files:\n",
        "#                     if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "#                         img_path = os.path.join(path, img_file)\n",
        "#                         self.images.append(img_path)\n",
        "#                         self.labels.append(self.class_to_idx[class_name])\n",
        "\n",
        "#         print(f\"Found {len(self.classes)} classes and {len(self.images)} images\")\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.images)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         img_path = self.images[idx]\n",
        "#         image = Image.open(img_path).convert('RGB')\n",
        "#         label = self.labels[idx]\n",
        "\n",
        "#         if self.transform:\n",
        "#             image = self.transform(image)\n",
        "\n",
        "#         # For multi-class classification, use a one-hot encoded vector\n",
        "#         # For the AttributePredictor model you showed earlier\n",
        "#         one_hot = torch.zeros(len(self.classes))\n",
        "#         one_hot[label] = 1.0\n",
        "\n",
        "#         return image, one_hot\n",
        "\n",
        "# # Alternative implementation for multi-label scenarios where images might have multiple attributes\n",
        "# class SUNMultiLabelDataset(Dataset):\n",
        "#     def __init__(self, root_dir, images_metadata, attributes_file=None, transform=None):\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             root_dir (string): Root directory with images\n",
        "#             attributes_file (string): Path to a file mapping images to multiple attributes\n",
        "#             transform (callable, optional): Optional transform to be applied on images\n",
        "#         \"\"\"\n",
        "#         self.root_dir = root_dir\n",
        "#         self.transform = transform\n",
        "#         self.images = []\n",
        "#         self.attributes = set()\n",
        "\n",
        "#         for image_metadata in images_metadata:\n",
        "#           self.images.append(image_metadata['path'])\n",
        "#           self.attr\n",
        "#         # Find all image files\n",
        "#         # self.images = []\n",
        "#         # for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
        "#         #     self.images.extend(glob.glob(os.path.join(root_dir, '**', ext), recursive=True))\n",
        "\n",
        "#         # # Get attribute names from directory structure\n",
        "#         # self.attributes = set()\n",
        "#         # for path in self.images:\n",
        "#         #     # Extract innermost directory name as attribute\n",
        "#         #     attr = os.path.basename(os.path.dirname(path))\n",
        "#         #     self.attributes.add(attr)\n",
        "\n",
        "#         # self.attributes = sorted(list(self.attributes))\n",
        "#         self.attr_to_idx = {attr: idx for idx, attr in enumerate(self.attributes)}\n",
        "\n",
        "#         # For each image, determine its attributes (using directory name)\n",
        "#         self.image_attrs = []\n",
        "#         for img_path in self.images:\n",
        "#             attr_name = os.path.basename(os.path.dirname(img_path))\n",
        "#             attr_vector = torch.zeros(len(self.attributes))\n",
        "#             attr_vector[self.attr_to_idx[attr_name]] = 1.0\n",
        "#             self.image_attrs.append(attr_vector)\n",
        "\n",
        "#         print(f\"Found {len(self.attributes)} attributes and {len(self.images)} images\")\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.images)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         img_path = self.images[idx]\n",
        "#         image = Image.open(img_path).convert('RGB')\n",
        "#         attrs = self.image_attrs[idx]\n",
        "\n",
        "#         if self.transform:\n",
        "#             image = self.transform(image)\n",
        "\n",
        "#         return image, attrs\n",
        "\n",
        "# # Example usage\n",
        "# def create_sun_dataloaders(root_dir, batch_size=32, train_split=0.8, num_workers=4):\n",
        "#     # Define transformations\n",
        "#     transform = transforms.Compose([\n",
        "#         transforms.Resize((224, 224)),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "#     ])\n",
        "\n",
        "#     # Create dataset\n",
        "#     # Choose the appropriate dataset class based on your needs\n",
        "#     # dataset = SUNDataset(root_dir, transform=transform)  # For single-label classification\n",
        "#     dataset = SUNMultiLabelDataset(root_dir, images_metadata=images_metadata, transform=transform)  # For multi-label classification\n",
        "\n",
        "#     # Split into train and test sets\n",
        "#     dataset_size = len(dataset)\n",
        "#     train_size = int(train_split * dataset_size)\n",
        "#     test_size = dataset_size - train_size\n",
        "#     train_dataset, test_dataset = random_split(\n",
        "#         dataset,\n",
        "#         [train_size, test_size],\n",
        "#         generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
        "#     )\n",
        "\n",
        "#     # Create data loaders\n",
        "#     train_dataloader = DataLoader(\n",
        "#         train_dataset,\n",
        "#         batch_size=batch_size,\n",
        "#         shuffle=True,\n",
        "#         num_workers=num_workers\n",
        "#     )\n",
        "\n",
        "#     test_dataloader = DataLoader(\n",
        "#         test_dataset,\n",
        "#         batch_size=batch_size,\n",
        "#         shuffle=False,\n",
        "#         num_workers=num_workers\n",
        "#     )\n",
        "\n",
        "#     return train_dataloader, test_dataloader, dataset.attributes if hasattr(dataset, 'attributes') else dataset.classes\n",
        "\n",
        "# # Example:\n",
        "# train_dataloader, test_dataloader, attribute_names = create_sun_dataloaders(\"./1/SUN/images/\")"
      ],
      "metadata": {
        "id": "I4e3GujtNOG5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT"
      ],
      "metadata": {
        "id": "RKoUJfnIUbko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Symbolic ViT\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class AttributePredictorViT(nn.Module):\n",
        "    def __init__(self, num_attributes=102):\n",
        "        super().__init__()\n",
        "        # Load a pretrained ViT model\n",
        "        self.backbone = models.vit_b_16(pretrained=True)\n",
        "\n",
        "        # Replace the classification head with a new one for attribute prediction\n",
        "        # ViT's hidden dimension is typically 768 for base models\n",
        "        hidden_size = self.backbone.heads.head.in_features\n",
        "        self.backbone.heads.head = nn.Linear(hidden_size, num_attributes)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sigmoid(self.backbone(x))\n",
        "\n",
        "# Initialize model, optimizer, and loss function\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_vit = AttributePredictorViT(num_attributes=102).to(device)\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for multi-label\n",
        "optimizer = torch.optim.Adam(model_vit.parameters(), lr=0.0001)  # Lower learning rate for transformer\n",
        "\n",
        "# Data preparation for ViT\n",
        "# Note: ViT typically uses a different input size (224x224 or 384x384)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train(model, dataloader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).int()\n",
        "            total += labels.size(0) * labels.size(1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        epoch_acc = correct / total\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).int()\n",
        "            total += labels.size(0) * labels.size(1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_loss = running_loss / len(dataloader)\n",
        "    test_acc = correct / total\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "    return test_loss, test_acc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU0A4EWFxxzk",
        "outputId": "598046fe-214a-43c5-a163-1c2f0fa1aae4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training and Evaluation\n",
        "train(model_vit, train_dataloader, criterion, optimizer, epochs=10)\n",
        "evaluate(model_vit, val_dataloader, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbDgY6oxJ4WV",
        "outputId": "c3819ebc-3f6a-43a6-b98c-26a62c076f83"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.2165, Accuracy: 0.8550\n",
            "Epoch 2/10, Loss: 0.1694, Accuracy: 0.8629\n",
            "Epoch 3/10, Loss: 0.1564, Accuracy: 0.8654\n",
            "Epoch 4/10, Loss: 0.1475, Accuracy: 0.8674\n",
            "Epoch 5/10, Loss: 0.1398, Accuracy: 0.8690\n",
            "Epoch 6/10, Loss: 0.1322, Accuracy: 0.8704\n",
            "Epoch 7/10, Loss: 0.1244, Accuracy: 0.8715\n",
            "Epoch 8/10, Loss: 0.1167, Accuracy: 0.8721\n",
            "Epoch 9/10, Loss: 0.1100, Accuracy: 0.8724\n",
            "Epoch 10/10, Loss: 0.1051, Accuracy: 0.8725\n",
            "Test Loss: 0.1779, Test Accuracy: 0.8614\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.1778793211704419, 0.8613851259205383)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_attributes(model, image_path, transform):\n",
        "    from PIL import Image\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)\n",
        "        predicted_attributes = (output)  # Binarize to 0/1\n",
        "\n",
        "    return predicted_attributes"
      ],
      "metadata": {
        "id": "clrvLyQ4WkJg"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "========================================================================"
      ],
      "metadata": {
        "id": "SSYU4mZGxv9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = predict_attributes(model_vit, '../test_images/access_road/sun_ahsirvbzddqizrxd.jpg', transform)"
      ],
      "metadata": {
        "id": "CXivaz87eurr"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((predicted[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lke_9HwghQvC",
        "outputId": "cc2f4853-0f6b-4874-9c67-3b8befc3e7c4"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5.9734e-03, 7.4273e-01, 9.6145e-01, 1.1458e-01, 1.4784e-02, 1.0612e-01,\n",
            "        9.5110e-02, 4.9456e-03, 1.0211e-01, 6.9229e-04, 2.2482e-04, 9.7374e-04,\n",
            "        1.4621e-04, 2.3633e-03, 3.6206e-03, 3.9846e-03, 1.3468e-03, 1.7799e-03,\n",
            "        3.6996e-03, 1.5969e-03, 2.5863e-04, 7.9773e-04, 1.9799e-03, 2.5594e-01,\n",
            "        7.0894e-03, 2.2080e-04, 3.1911e-04, 1.3924e-02, 3.3323e-04, 5.6394e-04,\n",
            "        2.9784e-04, 1.7219e-03, 9.7940e-04, 2.7881e-03, 3.6101e-04, 2.3289e-03,\n",
            "        3.0986e-02, 1.1113e-03, 2.0421e-03, 2.2363e-03, 9.1894e-01, 5.2473e-01,\n",
            "        9.4162e-01, 5.8994e-01, 9.9090e-01, 8.2765e-01, 1.1769e-02, 6.3314e-01,\n",
            "        7.8331e-01, 5.9926e-03, 2.7297e-04, 2.4185e-03, 3.0272e-03, 3.3442e-01,\n",
            "        2.5894e-03, 7.1942e-04, 1.3867e-03, 1.5570e-03, 3.5362e-03, 3.1773e-03,\n",
            "        2.0758e-02, 1.5256e-02, 1.5415e-01, 5.7359e-03, 1.2457e-03, 6.1586e-04,\n",
            "        9.0515e-04, 4.7326e-03, 1.8360e-03, 1.4697e-03, 1.8509e-03, 9.4468e-03,\n",
            "        1.0100e-03, 5.3198e-04, 9.3906e-01, 4.7598e-01, 2.9941e-04, 4.9603e-02,\n",
            "        5.9200e-03, 7.5616e-02, 1.6187e-03, 1.3190e-01, 1.3934e-01, 3.0900e-02,\n",
            "        2.7186e-03, 9.5245e-02, 1.3813e-02, 2.9482e-01, 5.7166e-01, 7.0156e-01,\n",
            "        1.0107e-02, 5.7167e-04, 1.2098e-02, 4.6576e-01, 4.5496e-03, 1.9670e-02,\n",
            "        7.1767e-02, 4.0403e-02, 4.5042e-04, 3.4358e-03, 1.9235e-01, 1.2531e-03],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ignore\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class AttributePredictor(nn.Module):\n",
        "    def __init__(self, num_attributes=102, extract_features=False):\n",
        "        super().__init__()\n",
        "        # Load Vision Transformer model with proper weights\n",
        "        self.backbone = models.vit_b_16(weights='ViT_B_16_Weights.DEFAULT')\n",
        "        self.hidden_dim = self.backbone.hidden_dim  # Usually 768 for ViT-B/16\n",
        "\n",
        "        # Replace classification head with our multi-label head\n",
        "        self.backbone.heads = nn.Linear(self.hidden_dim, num_attributes)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.extract_features = extract_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.extract_features:\n",
        "            # Normal prediction mode - full forward pass with sigmoid activation\n",
        "            x = self.backbone(x)\n",
        "            return self.sigmoid(x)\n",
        "        else:\n",
        "            # Feature extraction mode - get the features before classification head\n",
        "            # Process input and get patch embeddings\n",
        "            x = self.backbone.conv_proj(x)\n",
        "            x = x.flatten(2).transpose(1, 2)\n",
        "            x = self.backbone._process_input(x)\n",
        "\n",
        "            # Add class token and position embedding\n",
        "            cls_token = self.backbone.class_token.expand(x.shape[0], -1, -1)\n",
        "            x = torch.cat([cls_token, x], dim=1)\n",
        "            x = x + self.backbone.encoder.pos_embedding\n",
        "\n",
        "            # Pass through transformer encoder\n",
        "            x = self.backbone.encoder.dropout(x)\n",
        "            x = self.backbone.encoder.layers(x)\n",
        "\n",
        "            # Return the [CLS] token features only\n",
        "            return x[:, 0]\n",
        "\n",
        "# Initialize model, optimizer, and loss function\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AttributePredictor(num_attributes=102).to(device)\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for multi-label\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Data preparation with appropriate transforms for ViT\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Training function\n",
        "def train(model, dataloader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).int()\n",
        "            total += labels.size(0) * labels.size(1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        epoch_acc = correct / total\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).int()\n",
        "            total += labels.size(0) * labels.size(1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_loss = running_loss / len(dataloader)\n",
        "    test_acc = correct / total\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "    return test_loss, test_acc\n",
        "\n",
        "# Feature extraction function\n",
        "def extract_features(model, image_path, transform):\n",
        "    from PIL import Image\n",
        "\n",
        "    # Create a copy of the model and set to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Set feature extraction mode off - we'll extract at the right point manually\n",
        "    model.extract_features = False\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Use a hook to capture the output of the transformer encoder\n",
        "        features = None\n",
        "\n",
        "        def hook_fn(module, input, output):\n",
        "            nonlocal features\n",
        "            # Capture the [CLS] token (first token)\n",
        "            features = output[:, 0].detach()\n",
        "\n",
        "        # Register the hook on the last layer of the encoder\n",
        "        hook = model.backbone.encoder.layers[-1].register_forward_hook(hook_fn)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        _ = model(image_tensor)  # We'll capture the output with our hook\n",
        "\n",
        "        # Remove the hook\n",
        "        hook.remove()\n",
        "\n",
        "    return features.cpu().numpy()\n",
        "\n",
        "# Inference for a single image\n",
        "def predict_attributes(model, image_path, transform):\n",
        "    from PIL import Image\n",
        "\n",
        "    # Ensure model is in prediction mode\n",
        "    model.extract_features = False\n",
        "    model.eval()\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)\n",
        "        predicted_attributes = (output > 0.5).int()  # Binarize to 0/1\n",
        "\n",
        "    return predicted_attributes\n",
        "\n",
        "# Example usage:\n",
        "train(model, train_dataloader, criterion, optimizer, epochs=10)\n",
        "# test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
        "# predicted_attrs = predict_attributes(model, \"path/to/image.jpg\", transform)\n",
        "# feature_vector = extract_features(model, \"/content/1/SUN/images/a/abbey/sun_aacphuqehdodwawg.jpg\", transform)"
      ],
      "metadata": {
        "id": "WwDqG1OqUdwG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFWeRmwTQPVW",
        "outputId": "f86ac69e-4725-4e0a-85a8-e4f2a65591ba"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_vit, 'vit_symbolic_true.pth')"
      ],
      "metadata": {
        "id": "SqeMcXVkQG8Y"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp vit_symbolic_true.pth /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "EkueOxgSQ5ZI"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1-NN Dense Features\n",
        "import numpy as np\n",
        "from numpy.linalg import norm # Import norm specifically for clarity\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"\n",
        "    Calculates the cosine similarity between two vectors.\n",
        "\n",
        "    Args:\n",
        "        vec1 (list or np.ndarray): The first vector.\n",
        "        vec2 (list or np.ndarray): The second vector.\n",
        "\n",
        "    Returns:\n",
        "        float: The cosine similarity between the two vectors.\n",
        "               Returns 0 if either vector has zero magnitude.\n",
        "    \"\"\"\n",
        "    # Convert to numpy arrays if they aren't already\n",
        "    v1 = np.array(vec1)\n",
        "    v2 = np.array(vec2)\n",
        "\n",
        "    # Calculate dot product\n",
        "    dot_product = np.dot(v1, v2)\n",
        "\n",
        "    # Calculate L2 norms (magnitudes)\n",
        "    norm_v1 = norm(v1)\n",
        "    norm_v2 = norm(v2)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if norm_v1 == 0 or norm_v2 == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity = dot_product / (norm_v1 * norm_v2)\n",
        "\n",
        "    return similarity\n",
        "\n",
        "def retrieve_nearest_case_cosine(query_vector, case_base):\n",
        "    \"\"\"\n",
        "    Retrieves the nearest case from the case base using cosine similarity.\n",
        "\n",
        "    Args:\n",
        "        query_vector (list): The vector representing the query.\n",
        "        case_base (list): A list of dictionaries, where each dictionary\n",
        "                          represents a case and has an 'attributes' key\n",
        "                          containing a vector.\n",
        "\n",
        "    Returns:\n",
        "        dict: The case from case_base with the highest cosine similarity\n",
        "              to the query_vector, or None if case_base is empty.\n",
        "    \"\"\"\n",
        "    best_case = None\n",
        "    best_score = -1.0 # Cosine similarity ranges from -1 to 1\n",
        "\n",
        "    # Convert query_vector to numpy array once outside the loop for efficiency\n",
        "    query_np = np.array(query_vector)\n",
        "\n",
        "    for case in case_base:\n",
        "        # Ensure case attributes are also treated as numpy arrays\n",
        "        case_attributes_np = np.array(case['features_vector'])\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        # Pass numpy arrays directly to the helper function\n",
        "        sim = cosine_similarity(query_np, case_attributes_np)\n",
        "\n",
        "        # Update best case if current similarity is higher\n",
        "        if sim > best_score:\n",
        "            best_score = sim\n",
        "            best_case = case\n",
        "\n",
        "    return best_case\n",
        "\n",
        "nearest = retrieve_nearest_case_cosine(features, images_deep_features)\n",
        "print(f\"Nearest case based on cosine similarity: {nearest}\")"
      ],
      "metadata": {
        "id": "cY3saUz4nnN2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1-NN Symbolic AI\n",
        "import numpy as np\n",
        "from numpy.linalg import norm # Import norm specifically for clarity\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"\n",
        "    Calculates the cosine similarity between two vectors.\n",
        "\n",
        "    Args:\n",
        "        vec1 (list or np.ndarray): The first vector.\n",
        "        vec2 (list or np.ndarray): The second vector.\n",
        "\n",
        "    Returns:\n",
        "        float: The cosine similarity between the two vectors.\n",
        "               Returns 0 if either vector has zero magnitude.\n",
        "    \"\"\"\n",
        "    # Convert to numpy arrays if they aren't already\n",
        "    v1 = np.array(vec1)\n",
        "    v2 = np.array(vec2)\n",
        "\n",
        "    # Calculate dot product\n",
        "    dot_product = np.dot(v1, v2)\n",
        "\n",
        "    # Calculate L2 norms (magnitudes)\n",
        "    norm_v1 = norm(v1)\n",
        "    norm_v2 = norm(v2)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if norm_v1 == 0 or norm_v2 == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity = dot_product / (norm_v1 * norm_v2)\n",
        "\n",
        "    return similarity\n",
        "\n",
        "def retrieve_nearest_case_cosine(query_vector, case_base):\n",
        "    \"\"\"\n",
        "    Retrieves the nearest case from the case base using cosine similarity.\n",
        "\n",
        "    Args:\n",
        "        query_vector (list): The vector representing the query.\n",
        "        case_base (list): A list of dictionaries, where each dictionary\n",
        "                          represents a case and has an 'attributes' key\n",
        "                          containing a vector.\n",
        "\n",
        "    Returns:\n",
        "        dict: The case from case_base with the highest cosine similarity\n",
        "              to the query_vector, or None if case_base is empty.\n",
        "    \"\"\"\n",
        "    best_case = None\n",
        "    best_score = -1.0 # Cosine similarity ranges from -1 to 1\n",
        "\n",
        "    # Convert query_vector to numpy array once outside the loop for efficiency\n",
        "    query_np = query_vector.cpu().numpy() if hasattr(query_vector, 'cpu') else np.array(query_vector)\n",
        "\n",
        "    for case in case_base:\n",
        "        # Ensure case attributes are also treated as numpy arrays\n",
        "        case_attributes_np = np.array(case['labels_attr'])\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        # Pass numpy arrays directly to the helper function\n",
        "        sim = cosine_similarity(query_np, case_attributes_np)\n",
        "\n",
        "        # Update best case if current similarity is higher\n",
        "        if sim > best_score:\n",
        "            best_score = sim\n",
        "            best_case = case\n",
        "\n",
        "    return best_case\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rCXmCqrEk9TH"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predicted[0].cpu().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBvgqRwTmCUX",
        "outputId": "05586faf-1592-4308-83a5-d77322f5a9a5"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(images_metadata[0])\n",
        "nearest, confidence = predict_from_knn(predicted[0], images_metadata, k = 5)\n",
        "print(f\"Nearest case based on cosine similarity: {nearest}, confidence: {confidence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkBtJMagkQjV",
        "outputId": "f8714477-da69-4448-91b9-fa88079ba1fe"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest case based on cosine similarity: planetarium_outdoor, confidence: 0.880941470823506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title KNN Symbolic AI\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import heapq  # For efficient k-smallest elements handling\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"\n",
        "    Calculates the cosine similarity between two vectors.\n",
        "\n",
        "    Args:\n",
        "        vec1 (list or np.ndarray): The first vector.\n",
        "        vec2 (list or np.ndarray): The second vector.\n",
        "\n",
        "    Returns:\n",
        "        float: The cosine similarity between the two vectors.\n",
        "               Returns 0 if either vector has zero magnitude.\n",
        "    \"\"\"\n",
        "    # Convert to numpy arrays if they aren't already\n",
        "    v1 = np.array(vec1)\n",
        "    v2 = np.array(vec2)\n",
        "\n",
        "    # Calculate dot product\n",
        "    dot_product = np.dot(v1, v2)\n",
        "\n",
        "    # Calculate L2 norms (magnitudes)\n",
        "    norm_v1 = norm(v1)\n",
        "    norm_v2 = norm(v2)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if norm_v1 == 0 or norm_v2 == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity = dot_product / (norm_v1 * norm_v2)\n",
        "\n",
        "    return similarity\n",
        "\n",
        "def retrieve_k_nearest_cases_cosine(query_vector, case_base, k=3):\n",
        "    \"\"\"\n",
        "    Retrieves the k nearest cases from the case base using cosine similarity.\n",
        "\n",
        "    Args:\n",
        "        query_vector (list or np.ndarray): The vector representing the query.\n",
        "        case_base (list): A list of dictionaries, where each dictionary\n",
        "                          represents a case and has a 'labels_attr' key\n",
        "                          containing a vector.\n",
        "        k (int): The number of nearest neighbors to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of (case, similarity_score) tuples containing the k cases\n",
        "              with highest cosine similarity to the query_vector, sorted\n",
        "              in descending order of similarity.\n",
        "    \"\"\"\n",
        "    if not case_base:\n",
        "        return []\n",
        "\n",
        "    # Cap k to the size of the case base\n",
        "    k = min(k, len(case_base))\n",
        "\n",
        "    # Convert query_vector to numpy array once outside the loop for efficiency\n",
        "    query_np = query_vector.cpu().numpy() if hasattr(query_vector, 'cpu') else np.array(query_vector)\n",
        "\n",
        "    # Calculate similarities for all cases first\n",
        "    all_similarities = []\n",
        "\n",
        "    for i, case in enumerate(case_base):\n",
        "        # Ensure case attributes are also treated as numpy arrays\n",
        "        case_attributes_np = np.array(case['labels_attr'])\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        sim = cosine_similarity(query_np, case_attributes_np)\n",
        "\n",
        "        # Store similarity with case index\n",
        "        # We use negative similarity for the min-heap to function as a max-heap\n",
        "        all_similarities.append((-sim, i))\n",
        "\n",
        "    # Get k nearest neighbors using heapq\n",
        "    k_nearest_indices = heapq.nsmallest(k, all_similarities)\n",
        "\n",
        "    # Format results as [(case, similarity)]\n",
        "    result = [(case_base[idx], -sim) for sim, idx in k_nearest_indices]\n",
        "\n",
        "    return result\n",
        "\n",
        "def predict_from_knn(query_vector, case_base, k=3):\n",
        "    \"\"\"\n",
        "    Makes a prediction based on the k nearest neighbors.\n",
        "\n",
        "    Args:\n",
        "        query_vector (list or np.ndarray): The vector representing the query.\n",
        "        case_base (list): A list of dictionaries, where each dictionary\n",
        "                          represents a case with 'labels_attr' and 'label' keys.\n",
        "        k (int): The number of nearest neighbors to consider.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (predicted_label, confidence_score)\n",
        "               The predicted label is based on majority voting.\n",
        "               The confidence score is the average similarity of the k neighbors.\n",
        "    \"\"\"\n",
        "    neighbors = retrieve_k_nearest_cases_cosine(query_vector, case_base, k)\n",
        "\n",
        "    if not neighbors:\n",
        "        return None, 0.0\n",
        "\n",
        "    # Count votes for each label\n",
        "    votes = {}\n",
        "    total_similarity = 0.0\n",
        "\n",
        "    for case, similarity in neighbors:\n",
        "        label = case.get('label')\n",
        "        if label not in votes:\n",
        "            votes[label] = 0\n",
        "        votes[label] += 1\n",
        "        total_similarity += similarity\n",
        "\n",
        "    # Find label with most votes\n",
        "    predicted_label = max(votes.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    # Calculate confidence score (average similarity)\n",
        "    confidence_score = total_similarity / len(neighbors)\n",
        "\n",
        "    return predicted_label, confidence_score\n",
        "\n",
        "# Example usage\n",
        "# Create a case base with attribute vectors and labels\n",
        "# case_base = [\n",
        "#     {'labels_attr': [1, 0, 1, 0], 'label': 'A'},\n",
        "#     {'labels_attr': [0, 1, 0, 1], 'label': 'B'},\n",
        "#     {'labels_attr': [1, 1, 0, 0], 'label': 'A'},\n",
        "#     {'labels_attr': [0, 0, 1, 1], 'label': 'B'},\n",
        "#     {'labels_attr': [1, 0, 0, 1], 'label': 'C'},\n",
        "# ]\n",
        "\n",
        "# Query vector\n",
        "# query = [0.9, 0.1, 0.8, 0.2]\n",
        "\n",
        "# # Get 3 nearest neighbors\n",
        "# neighbors = retrieve_k_nearest_cases_cosine(query, case_base, k=3)\n",
        "# print(\"Top 3 neighbors:\")\n",
        "# for case, similarity in neighbors:\n",
        "#     print(f\"Case: {case}, Similarity: {similarity:.4f}\")\n",
        "\n",
        "# # Make a prediction\n",
        "# prediction, confidence = predict_from_knn(query, case_base, k=3)\n",
        "# print(f\"Prediction: {prediction}, Confidence: {confidence:.4f}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pJmDUSQ0nuIM"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = predict_attributes(model_vit, '../test_images/access_road/sun_ahsirvbzddqizrxd.jpg', transform)"
      ],
      "metadata": {
        "id": "E0A45PcSwpJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Get All Test Files\n",
        "# Option 1: Using os module\n",
        "import os\n",
        "\n",
        "# Option 2: Using pathlib (more modern approach)\n",
        "from pathlib import Path\n",
        "\n",
        "def traverse_directory_pathlib(directory_path):\n",
        "    \"\"\"\n",
        "    Traverse all files in the specified directory and its subdirectories using pathlib.\n",
        "\n",
        "    Args:\n",
        "        directory_path (str): Path to the directory to traverse\n",
        "\n",
        "    Returns:\n",
        "        list: A list of Path objects for all files found\n",
        "    \"\"\"\n",
        "    all_files = []\n",
        "    directory = Path(directory_path)\n",
        "\n",
        "    try:\n",
        "        # Check if the directory exists\n",
        "        if not directory.exists():\n",
        "            print(f\"Directory does not exist: {directory_path}\")\n",
        "            return all_files\n",
        "\n",
        "        # Recursively iterate through all files\n",
        "        for file_path in directory.glob('**/*'):\n",
        "            if file_path.is_file():\n",
        "                all_files.append(file_path)\n",
        "\n",
        "                # Print file information\n",
        "                # print(f\"Found file: {file_path}\")\n",
        "                # print(f\"  - Size: {file_path.stat().st_size} bytes\")\n",
        "                # print(f\"  - Type: {file_path.suffix}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error traversing directory: {e}\")\n",
        "\n",
        "    return all_files\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace \"../test_images\" with your actual directory path if needed\n",
        "    directory_path = \"../test_images\"\n",
        "\n",
        "    files_pathlib = traverse_directory_pathlib(directory_path)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "40cpnYPlxp3J"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(files_pathlib[34])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRAtHrSax41t",
        "outputId": "66eeda65-151b-418e-f51d-11359e96a784"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "../test_images/cathedral_outdoor/sun_atadimdgdmjvnizz.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_examples = 0\n",
        "matches = 0\n",
        "for f in files_pathlib:\n",
        "  # print(\"file_path: \", f)\n",
        "  real_category = str(f).split('/')[-2]\n",
        "  predicted = predict_attributes(model_vit, f, transform)\n",
        "  nearest, confidence = predict_from_knn(predicted[0], images_metadata, k = 20)\n",
        "  if (real_category == nearest): matches += 1\n",
        "  # print(f\"Nearest case based on cosine similarity: {nearest}, confidence: {confidence}\")\n",
        "  total_examples += 1\n",
        "  # if (total_examples == 10): break\n",
        "  print(total_examples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec41qmgNyqCc",
        "outputId": "7f69bec9-4ebb-4737-da9c-3393bd9725ea"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n",
            "1000\n",
            "1001\n",
            "1002\n",
            "1003\n",
            "1004\n",
            "1005\n",
            "1006\n",
            "1007\n",
            "1008\n",
            "1009\n",
            "1010\n",
            "1011\n",
            "1012\n",
            "1013\n",
            "1014\n",
            "1015\n",
            "1016\n",
            "1017\n",
            "1018\n",
            "1019\n",
            "1020\n",
            "1021\n",
            "1022\n",
            "1023\n",
            "1024\n",
            "1025\n",
            "1026\n",
            "1027\n",
            "1028\n",
            "1029\n",
            "1030\n",
            "1031\n",
            "1032\n",
            "1033\n",
            "1034\n",
            "1035\n",
            "1036\n",
            "1037\n",
            "1038\n",
            "1039\n",
            "1040\n",
            "1041\n",
            "1042\n",
            "1043\n",
            "1044\n",
            "1045\n",
            "1046\n",
            "1047\n",
            "1048\n",
            "1049\n",
            "1050\n",
            "1051\n",
            "1052\n",
            "1053\n",
            "1054\n",
            "1055\n",
            "1056\n",
            "1057\n",
            "1058\n",
            "1059\n",
            "1060\n",
            "1061\n",
            "1062\n",
            "1063\n",
            "1064\n",
            "1065\n",
            "1066\n",
            "1067\n",
            "1068\n",
            "1069\n",
            "1070\n",
            "1071\n",
            "1072\n",
            "1073\n",
            "1074\n",
            "1075\n",
            "1076\n",
            "1077\n",
            "1078\n",
            "1079\n",
            "1080\n",
            "1081\n",
            "1082\n",
            "1083\n",
            "1084\n",
            "1085\n",
            "1086\n",
            "1087\n",
            "1088\n",
            "1089\n",
            "1090\n",
            "1091\n",
            "1092\n",
            "1093\n",
            "1094\n",
            "1095\n",
            "1096\n",
            "1097\n",
            "1098\n",
            "1099\n",
            "1100\n",
            "1101\n",
            "1102\n",
            "1103\n",
            "1104\n",
            "1105\n",
            "1106\n",
            "1107\n",
            "1108\n",
            "1109\n",
            "1110\n",
            "1111\n",
            "1112\n",
            "1113\n",
            "1114\n",
            "1115\n",
            "1116\n",
            "1117\n",
            "1118\n",
            "1119\n",
            "1120\n",
            "1121\n",
            "1122\n",
            "1123\n",
            "1124\n",
            "1125\n",
            "1126\n",
            "1127\n",
            "1128\n",
            "1129\n",
            "1130\n",
            "1131\n",
            "1132\n",
            "1133\n",
            "1134\n",
            "1135\n",
            "1136\n",
            "1137\n",
            "1138\n",
            "1139\n",
            "1140\n",
            "1141\n",
            "1142\n",
            "1143\n",
            "1144\n",
            "1145\n",
            "1146\n",
            "1147\n",
            "1148\n",
            "1149\n",
            "1150\n",
            "1151\n",
            "1152\n",
            "1153\n",
            "1154\n",
            "1155\n",
            "1156\n",
            "1157\n",
            "1158\n",
            "1159\n",
            "1160\n",
            "1161\n",
            "1162\n",
            "1163\n",
            "1164\n",
            "1165\n",
            "1166\n",
            "1167\n",
            "1168\n",
            "1169\n",
            "1170\n",
            "1171\n",
            "1172\n",
            "1173\n",
            "1174\n",
            "1175\n",
            "1176\n",
            "1177\n",
            "1178\n",
            "1179\n",
            "1180\n",
            "1181\n",
            "1182\n",
            "1183\n",
            "1184\n",
            "1185\n",
            "1186\n",
            "1187\n",
            "1188\n",
            "1189\n",
            "1190\n",
            "1191\n",
            "1192\n",
            "1193\n",
            "1194\n",
            "1195\n",
            "1196\n",
            "1197\n",
            "1198\n",
            "1199\n",
            "1200\n",
            "1201\n",
            "1202\n",
            "1203\n",
            "1204\n",
            "1205\n",
            "1206\n",
            "1207\n",
            "1208\n",
            "1209\n",
            "1210\n",
            "1211\n",
            "1212\n",
            "1213\n",
            "1214\n",
            "1215\n",
            "1216\n",
            "1217\n",
            "1218\n",
            "1219\n",
            "1220\n",
            "1221\n",
            "1222\n",
            "1223\n",
            "1224\n",
            "1225\n",
            "1226\n",
            "1227\n",
            "1228\n",
            "1229\n",
            "1230\n",
            "1231\n",
            "1232\n",
            "1233\n",
            "1234\n",
            "1235\n",
            "1236\n",
            "1237\n",
            "1238\n",
            "1239\n",
            "1240\n",
            "1241\n",
            "1242\n",
            "1243\n",
            "1244\n",
            "1245\n",
            "1246\n",
            "1247\n",
            "1248\n",
            "1249\n",
            "1250\n",
            "1251\n",
            "1252\n",
            "1253\n",
            "1254\n",
            "1255\n",
            "1256\n",
            "1257\n",
            "1258\n",
            "1259\n",
            "1260\n",
            "1261\n",
            "1262\n",
            "1263\n",
            "1264\n",
            "1265\n",
            "1266\n",
            "1267\n",
            "1268\n",
            "1269\n",
            "1270\n",
            "1271\n",
            "1272\n",
            "1273\n",
            "1274\n",
            "1275\n",
            "1276\n",
            "1277\n",
            "1278\n",
            "1279\n",
            "1280\n",
            "1281\n",
            "1282\n",
            "1283\n",
            "1284\n",
            "1285\n",
            "1286\n",
            "1287\n",
            "1288\n",
            "1289\n",
            "1290\n",
            "1291\n",
            "1292\n",
            "1293\n",
            "1294\n",
            "1295\n",
            "1296\n",
            "1297\n",
            "1298\n",
            "1299\n",
            "1300\n",
            "1301\n",
            "1302\n",
            "1303\n",
            "1304\n",
            "1305\n",
            "1306\n",
            "1307\n",
            "1308\n",
            "1309\n",
            "1310\n",
            "1311\n",
            "1312\n",
            "1313\n",
            "1314\n",
            "1315\n",
            "1316\n",
            "1317\n",
            "1318\n",
            "1319\n",
            "1320\n",
            "1321\n",
            "1322\n",
            "1323\n",
            "1324\n",
            "1325\n",
            "1326\n",
            "1327\n",
            "1328\n",
            "1329\n",
            "1330\n",
            "1331\n",
            "1332\n",
            "1333\n",
            "1334\n",
            "1335\n",
            "1336\n",
            "1337\n",
            "1338\n",
            "1339\n",
            "1340\n",
            "1341\n",
            "1342\n",
            "1343\n",
            "1344\n",
            "1345\n",
            "1346\n",
            "1347\n",
            "1348\n",
            "1349\n",
            "1350\n",
            "1351\n",
            "1352\n",
            "1353\n",
            "1354\n",
            "1355\n",
            "1356\n",
            "1357\n",
            "1358\n",
            "1359\n",
            "1360\n",
            "1361\n",
            "1362\n",
            "1363\n",
            "1364\n",
            "1365\n",
            "1366\n",
            "1367\n",
            "1368\n",
            "1369\n",
            "1370\n",
            "1371\n",
            "1372\n",
            "1373\n",
            "1374\n",
            "1375\n",
            "1376\n",
            "1377\n",
            "1378\n",
            "1379\n",
            "1380\n",
            "1381\n",
            "1382\n",
            "1383\n",
            "1384\n",
            "1385\n",
            "1386\n",
            "1387\n",
            "1388\n",
            "1389\n",
            "1390\n",
            "1391\n",
            "1392\n",
            "1393\n",
            "1394\n",
            "1395\n",
            "1396\n",
            "1397\n",
            "1398\n",
            "1399\n",
            "1400\n",
            "1401\n",
            "1402\n",
            "1403\n",
            "1404\n",
            "1405\n",
            "1406\n",
            "1407\n",
            "1408\n",
            "1409\n",
            "1410\n",
            "1411\n",
            "1412\n",
            "1413\n",
            "1414\n",
            "1415\n",
            "1416\n",
            "1417\n",
            "1418\n",
            "1419\n",
            "1420\n",
            "1421\n",
            "1422\n",
            "1423\n",
            "1424\n",
            "1425\n",
            "1426\n",
            "1427\n",
            "1428\n",
            "1429\n",
            "1430\n",
            "1431\n",
            "1432\n",
            "1433\n",
            "1434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB_0ItVE1rRX",
        "outputId": "d403b0e6-0eb6-4f15-caba-31d189bf53f9"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "260\n"
          ]
        }
      ]
    }
  ]
}